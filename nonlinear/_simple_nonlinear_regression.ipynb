{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      WL1(mm)  WL2(mm)  WL3(mm)  WL4 (mm)  WL5 (mm)  Solar (W/m2)  \\\n",
      "0          65      213       94        37        43           415   \n",
      "1          65      211      101        37        43           329   \n",
      "2          66      211      102        36        43           115   \n",
      "3          64      212       88        36        43            31   \n",
      "4          65      210       95        36        43             1   \n",
      "...       ...      ...      ...       ...       ...           ...   \n",
      "1425       39       41       41        36        44           665   \n",
      "1426       39       41       41        36        43           557   \n",
      "1427       39       41       41        37        44           395   \n",
      "1428       39       41       41        36        43           208   \n",
      "1429       39       41       41        36        44            44   \n",
      "\n",
      "      Precipitation (mm)  AirTemp (DegC)  RH (%)  PD (mm)  \n",
      "0                  0.000            34.0      51     70.5  \n",
      "1                  0.000            33.5      67     72.7  \n",
      "2                  0.000            32.8      68     72.4  \n",
      "3                  0.000            30.8      78     72.5  \n",
      "4                  0.000            28.9      87     72.4  \n",
      "...                  ...             ...     ...      ...  \n",
      "1425               0.017            24.9      47     54.9  \n",
      "1426               0.017            24.9      44     54.4  \n",
      "1427               0.000            24.6      43     51.6  \n",
      "1428               0.017            23.9      44     48.6  \n",
      "1429               0.017            22.3      43     46.6  \n",
      "\n",
      "[1430 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "data_display = pd.read_csv(\"parsed_alldata.csv\")\n",
    "\n",
    "print(data_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "trainning_steps = 500000\n",
    "display_step = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(\"parsed_alldata_np.csv\", delimiter = ',', dtype = np.float32)\n",
    "\n",
    "x_train = data[:1200, [0]]\n",
    "y_train = data[:1200, [-1]]\n",
    "\n",
    "x_test = data[1200:, [0]]\n",
    "y_test = data[1200:, [-1]]\n",
    "\n",
    "w1 = tf.Variable(random.random())\n",
    "w2 = tf.Variable(random.random())\n",
    "bias = tf.Variable(random.random())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 10000 \n",
      " w1: -0.010298714 \n",
      " w2: 1.801851 \n",
      " bias: 3.8797476 \n",
      " loss: 65.27165 \n",
      "\n",
      "step: 20000 \n",
      " w1: -0.015148424 \n",
      " w2: 2.0444045 \n",
      " bias: 8.281346 \n",
      " loss: 16.78123 \n",
      "\n",
      "step: 30000 \n",
      " w1: -0.012607167 \n",
      " w2: 1.7156079 \n",
      " bias: 17.622631 \n",
      " loss: 12.405648 \n",
      "\n",
      "step: 40000 \n",
      " w1: -0.010154787 \n",
      " w2: 1.3982662 \n",
      " bias: 26.63945 \n",
      " loss: 9.253511 \n",
      "\n",
      "step: 50000 \n",
      " w1: -0.007877342 \n",
      " w2: 1.1064811 \n",
      " bias: 34.92908 \n",
      " loss: 7.288745 \n",
      "\n",
      "step: 60000 \n",
      " w1: -0.0060727233 \n",
      " w2: 0.8707149 \n",
      " bias: 41.62525 \n",
      " loss: 6.3431177 \n",
      "\n",
      "step: 70000 \n",
      " w1: -0.004944576 \n",
      " w2: 0.7243108 \n",
      " bias: 45.78151 \n",
      " loss: 6.050702 \n",
      "\n",
      "step: 80000 \n",
      " w1: -0.0044103498 \n",
      " w2: 0.65518165 \n",
      " bias: 47.743565 \n",
      " loss: 5.9904404 \n",
      "\n",
      "step: 90000 \n",
      " w1: -0.0041851588 \n",
      " w2: 0.62615794 \n",
      " bias: 48.567303 \n",
      " loss: 5.9799895 \n",
      "\n",
      "step: 100000 \n",
      " w1: -0.004094267 \n",
      " w2: 0.6142654 \n",
      " bias: 48.904774 \n",
      " loss: 5.9782543 \n",
      "\n",
      "step: 110000 \n",
      " w1: -0.004034733 \n",
      " w2: 0.60948086 \n",
      " bias: 49.041214 \n",
      " loss: 5.9822426 \n",
      "\n",
      "step: 120000 \n",
      " w1: -0.0040409807 \n",
      " w2: 0.60752016 \n",
      " bias: 49.096237 \n",
      " loss: 5.9779143 \n",
      "\n",
      "step: 130000 \n",
      " w1: -0.004034183 \n",
      " w2: 0.6067402 \n",
      " bias: 49.118393 \n",
      " loss: 5.9779096 \n",
      "\n",
      "step: 140000 \n",
      " w1: -0.00403423 \n",
      " w2: 0.60664374 \n",
      " bias: 49.12109 \n",
      " loss: 5.977901 \n",
      "\n",
      "step: 150000 \n",
      " w1: -0.0040336912 \n",
      " w2: 0.6065741 \n",
      " bias: 49.123066 \n",
      " loss: 5.9779067 \n",
      "\n",
      "step: 160000 \n",
      " w1: -0.0040331683 \n",
      " w2: 0.60650635 \n",
      " bias: 49.124992 \n",
      " loss: 5.9779015 \n",
      "\n",
      "step: 170000 \n",
      " w1: -0.0040326454 \n",
      " w2: 0.6064383 \n",
      " bias: 49.126926 \n",
      " loss: 5.9779 \n",
      "\n",
      "step: 180000 \n",
      " w1: -0.004032305 \n",
      " w2: 0.60639495 \n",
      " bias: 49.12816 \n",
      " loss: 5.9779043 \n",
      "\n",
      "step: 190000 \n",
      " w1: -0.0040320144 \n",
      " w2: 0.6063571 \n",
      " bias: 49.129234 \n",
      " loss: 5.9779067 \n",
      "\n",
      "step: 200000 \n",
      " w1: -0.0040318477 \n",
      " w2: 0.6063353 \n",
      " bias: 49.129852 \n",
      " loss: 5.977906 \n",
      "\n",
      "step: 210000 \n",
      " w1: -0.004031671 \n",
      " w2: 0.6063126 \n",
      " bias: 49.130497 \n",
      " loss: 5.977909 \n",
      "\n",
      "step: 220000 \n",
      " w1: -0.004031627 \n",
      " w2: 0.6062963 \n",
      " bias: 49.130955 \n",
      " loss: 5.9779034 \n",
      "\n",
      "step: 230000 \n",
      " w1: -0.0040314146 \n",
      " w2: 0.6062796 \n",
      " bias: 49.131435 \n",
      " loss: 5.9779067 \n",
      "\n",
      "step: 240000 \n",
      " w1: -0.0040313434 \n",
      " w2: 0.6062701 \n",
      " bias: 49.131706 \n",
      " loss: 5.977905 \n",
      "\n",
      "step: 250000 \n",
      " w1: -0.0040465835 \n",
      " w2: 0.60623133 \n",
      " bias: 49.13236 \n",
      " loss: 5.9801483 \n",
      "\n",
      "step: 260000 \n",
      " w1: -0.004031068 \n",
      " w2: 0.60623455 \n",
      " bias: 49.132717 \n",
      " loss: 5.9778996 \n",
      "\n",
      "step: 270000 \n",
      " w1: -0.0040309806 \n",
      " w2: 0.60622257 \n",
      " bias: 49.133057 \n",
      " loss: 5.9778996 \n",
      "\n",
      "step: 280000 \n",
      " w1: -0.0040308894 \n",
      " w2: 0.60621154 \n",
      " bias: 49.13337 \n",
      " loss: 5.977906 \n",
      "\n",
      "step: 290000 \n",
      " w1: -0.0040572896 \n",
      " w2: 0.606173 \n",
      " bias: 49.133698 \n",
      " loss: 5.984537 \n",
      "\n",
      "step: 300000 \n",
      " w1: -0.0040307576 \n",
      " w2: 0.60619444 \n",
      " bias: 49.133858 \n",
      " loss: 5.977903 \n",
      "\n",
      "step: 310000 \n",
      " w1: -0.0041782465 \n",
      " w2: 0.6060413 \n",
      " bias: 49.133865 \n",
      " loss: 6.183507 \n",
      "\n",
      "step: 320000 \n",
      " w1: -0.004031188 \n",
      " w2: 0.6061822 \n",
      " bias: 49.13419 \n",
      " loss: 5.97791 \n",
      "\n",
      "step: 330000 \n",
      " w1: -0.004340202 \n",
      " w2: 0.60587144 \n",
      " bias: 49.133923 \n",
      " loss: 6.8830695 \n",
      "\n",
      "step: 340000 \n",
      " w1: -0.0040306537 \n",
      " w2: 0.60618114 \n",
      " bias: 49.13423 \n",
      " loss: 5.97791 \n",
      "\n",
      "step: 350000 \n",
      " w1: -0.004032458 \n",
      " w2: 0.6061741 \n",
      " bias: 49.134377 \n",
      " loss: 5.977933 \n",
      "\n",
      "step: 360000 \n",
      " w1: -0.0040305927 \n",
      " w2: 0.606173 \n",
      " bias: 49.134464 \n",
      " loss: 5.977901 \n",
      "\n",
      "step: 370000 \n",
      " w1: -0.004017326 \n",
      " w2: 0.6061847 \n",
      " bias: 49.134518 \n",
      " loss: 5.9795623 \n",
      "\n",
      "step: 380000 \n",
      " w1: -0.0040313196 \n",
      " w2: 0.6061698 \n",
      " bias: 49.134537 \n",
      " loss: 5.9779086 \n",
      "\n",
      "step: 390000 \n",
      " w1: -0.0040260055 \n",
      " w2: 0.6061717 \n",
      " bias: 49.134632 \n",
      " loss: 5.978099 \n",
      "\n",
      "step: 400000 \n",
      " w1: -0.0040285117 \n",
      " w2: 0.6061677 \n",
      " bias: 49.134678 \n",
      " loss: 5.9779425 \n",
      "\n",
      "step: 410000 \n",
      " w1: -0.0040305164 \n",
      " w2: 0.6061632 \n",
      " bias: 49.134743 \n",
      " loss: 5.97791 \n",
      "\n",
      "step: 420000 \n",
      " w1: -0.004031146 \n",
      " w2: 0.6061614 \n",
      " bias: 49.134777 \n",
      " loss: 5.977905 \n",
      "\n",
      "step: 430000 \n",
      " w1: -0.0040304926 \n",
      " w2: 0.60616016 \n",
      " bias: 49.134827 \n",
      " loss: 5.977907 \n",
      "\n",
      "step: 440000 \n",
      " w1: -0.004030436 \n",
      " w2: 0.60615975 \n",
      " bias: 49.134846 \n",
      " loss: 5.977904 \n",
      "\n",
      "step: 450000 \n",
      " w1: -0.0040304284 \n",
      " w2: 0.6061601 \n",
      " bias: 49.13483 \n",
      " loss: 5.977904 \n",
      "\n",
      "step: 460000 \n",
      " w1: -0.0040304647 \n",
      " w2: 0.60616106 \n",
      " bias: 49.134804 \n",
      " loss: 5.977904 \n",
      "\n",
      "step: 470000 \n",
      " w1: -0.004030495 \n",
      " w2: 0.60616064 \n",
      " bias: 49.134815 \n",
      " loss: 5.9779043 \n",
      "\n",
      "step: 480000 \n",
      " w1: -0.004030488 \n",
      " w2: 0.6061595 \n",
      " bias: 49.13485 \n",
      " loss: 5.977904 \n",
      "\n",
      "step: 490000 \n",
      " w1: -0.004030472 \n",
      " w2: 0.60615754 \n",
      " bias: 49.134907 \n",
      " loss: 5.9779024 \n",
      "\n",
      "step: 500000 \n",
      " w1: -0.0040304717 \n",
      " w2: 0.6061564 \n",
      " bias: 49.134937 \n",
      " loss: 5.9779043 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def compute_loss():\n",
    "    hypothesis = w1 * x_train * x_train + w2 * x_train + bias\n",
    "    loss = tf.reduce_mean((y_train - hypothesis) ** 2)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "optimizer = tf.optimizers.Adam(lr = learning_rate)\n",
    "\n",
    "for step in range(1, trainning_steps + 1):\n",
    "    optimizer.minimize(compute_loss, var_list = [w1, w2, bias])\n",
    "    \n",
    "    if step % display_step == 0:\n",
    "        print('step:', step, '\\n', 'w1:', w1.numpy(), '\\n', 'w2:', w2.numpy(), '\\n', \n",
    "              'bias:', bias.numpy(), '\\n', 'loss:', compute_loss().numpy(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
